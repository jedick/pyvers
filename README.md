# pyvers
A Python package and app for training and running claim verification models.

## Features

- Web app
  - Based on LitServe and Gradio.
- Trainer
	- Based on PyTorch Lightning.
    - Use any [pretrained sequence classification model](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification) from HuggingFace.
    - Supports [shuffling training data](https://github.com/jedick/pyvers/blob/main/scripts/shuffle_datasets.py) from multiple datasets for [improved generalization performance](https://jedick.github.io/blog/experimenting-with-transformer-models/#cross-dataset-generalization) across datasets.
    - Logger is configured to plot training and validation loss on the [same graph in TensorBoard](https://jedick.github.io/blog/experimenting-with-transformer-models/#the-paradox-of-rising-loss-and-improving-accuracy).

## Running the app

There is no need to install pyvers to run the app.
The `pip install` command takes care of the necessary requirements for the app.
Then, run the two `python` commands in different terminals.

```
pip install torch transformers litserve gradio
python app/server.py
python app/app.py
```

App usage:

- Browse to the URL generated by the last command.
- Input a claim and evidence ([example](https://huggingface.co/datasets/nyu-mll/multi_nli/viewer/default/train?row=37&views%5B%5D=train)).
- Hit "Enter" or press the Submit button to run the inference.
- The probabilities predicted by the model are printed in the Classification text box and visualized in the barchart.
- Change the model using the dropdown at the top. This automatically re-runs the inference using the new model.

Screenshot:

![Screenshot of pyvers app](./assets/pyvers_app_screenshot.png)

## Installation

Install pyvers if you want to fine-tune models or use the data modules.
Supported datasets are [Fever](https://huggingface.co/datasets/fever/fever), [SciFact](https://github.com/allenai/scifact), [Citation-Integrity](https://github.com/ScienceNLP-Lab/Citation-Integrity/), and a small toy dataset.

These commands install the requirements, then the pyvers package in [development mode](https://setuptools.pypa.io/en/latest/userguide/development_mode.html) (remove the `-e` for a standard installation).

```
pip install -r requirements.txt
pip install -e pyvers
```
## Loading data

The `pyvers.data.FileDataModule` class loads data from local data files in JSON lines format (jsonl).
The schema for the data files is described [here](https://github.com/dwadden/multivers/blob/main/doc/data.md).
Get data files for SciFact and Citation-Integrity with labels used in pyvers [here](https://github.com/jedick/AI4citations/tree/main/data).
The data module can be used to shuffle training data from both datasets.

```python
from pyvers.data import FileDataModule
# Set the model used for the tokenizer
model_name = "bert-base-uncased"

# Load data from one dataset
dm = FileDataModule("data/scifact", model_name)

# Shuffle training data from two datasets
dm = FileDataModule(["data/scifact", "data/citint"], model_name)

# Get some tokenized data
dm.setup("fit")
next(iter(dm.train_dataloader()))
```

The `pyvers.data.NLIDataModule` class loads data from HuggingFace datasets.

```python
from pyvers.data import NLIDataModule
model_name = "bert-base-uncased"

# Load data from HuggingFace datasets
dm = NLIDataModule("facebook/anli", model_name)

# Get some tokenized data
dm.prepare_data()
dm.setup("fit")
next(iter(dm.train_dataloader()))
```

## Fine-tuning example

This takes about a minute on a CPU.

```python
# Import required modules
import pytorch_lightning as pl
from pyvers.data import ToyDataModule
from pyvers.model import PyversClassifier

# Initialize data and model
dm = ToyDataModule("bert-base-uncased")
model = PyversClassifier(dm.model_name)

# Train model
trainer = pl.Trainer(enable_checkpointing=False, max_epochs=20)
trainer.fit(model, datamodule=dm)

# Test model
trainer.test(model, datamodule=dm)

# Show predictions
predictions = trainer.predict(model, datamodule=dm)
print(predictions)
```

This is what we get (results vary between runs):

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        AUROC Macro        │          0.963            │
│      AUROC Weighted       │          0.963            │
│         Accuracy          │           88.9            │
│         F1 Macro          │           88.6            │
│         F1 Micro          │           88.9            │
│          F1_NEI           │          100.0            │
│         F1_REFUTE         │           80.0            │
│        F1_SUPPORT         │           85.7            │
└───────────────────────────┴───────────────────────────┘

[['SUPPORT', 'SUPPORT', 'SUPPORT', 'NEI', 'NEI', 'NEI', 'REFUTE', 'REFUTE', 'SUPPORT']]

# Ground-truth labels are:
# [['SUPPORT', 'SUPPORT', 'SUPPORT', 'NEI', 'NEI', 'NEI', 'REFUTE', 'REFUTE', 'REFUTE']]
```

## Zero-shot example

This uses a [DeBERTa model trained on MultiNLI, Fever-NLI and Adversarial-NLI (ANLI)](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) for zero-shot classification of claim-evidence pairs.

```
import pytorch_lightning as pl
from pyvers.model import PyversClassifier
from pyvers.data import ToyDataModule
dm = ToyDataModule("MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli")
model = PyversClassifier(dm.model_name)
trainer = pl.Trainer()
dm.setup(stage="test")
predictions = trainer.predict(model, datamodule=dm)
print(predictions)
# [['SUPPORT', 'SUPPORT', 'SUPPORT', 'REFUTE', 'REFUTE', 'REFUTE', 'REFUTE', 'REFUTE', 'REFUTE']]
```

The pretrained model successfully distinguishes between SUPPORT and REFUTE on the toy dataset but misclassifies NEI as REFUTE.
This can be improved with fine-tuning.


*When using a pre-trained model for zero-shot classification, check the mapping between labels and IDs.*

```python
from transformers import AutoConfig

model_name = "answerdotai/ModernBERT-base"
config = AutoConfig.from_pretrained(model_name, num_labels=3)
print(config.to_dict()["id2label"])
# {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'}

model_name = "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli"
config = AutoConfig.from_pretrained(model_name, num_labels=3)
print(config.to_dict()["id2label"])
# {0: 'entailment', 1: 'neutral', 2: 'contradiction'}
```

Because it uses labels that are consistent with the NLI categories listed below, for *zero-shot classification* we would choose the pretrained DeBERTa model rather than ModernBERT.
However, *fine-tuning* either model for text classification should work (see [this page](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-modern-bert-in-2025.ipynb) for information on fine-tuning ModernBERT).

## Label to ID mapping

| ID | pyvers  | [Fever](https://huggingface.co/datasets/fever/fever)* | [MultiNLI](https://huggingface.co/datasets/nyu-mll/multi_nli), [ANLI](https://huggingface.co/datasets/facebook/anli) |
| - | - | - | - |
| 0  | SUPPORT | SUPPORTS        | entailment |
| 1  | NEI     | NOT ENOUGH INFO | neutral |
| 2  | REFUTE  | REFUTES         | contradiction |

\* Text labels only

